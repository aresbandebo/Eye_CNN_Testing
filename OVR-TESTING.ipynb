{
 "cells": [
  {
   "cell_type": "code",
   "id": "74c5398ef0d8c133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T12:24:16.316694Z",
     "start_time": "2025-11-03T12:24:12.598390Z"
    }
   },
   "source": [
    "# Step 1: Import Necessary Libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# --- Constants and Configuration ---\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T12:24:16.333281Z",
     "start_time": "2025-11-03T12:24:16.326360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Create the classes for the images based on the folder names with different conditions in each folder\n",
    "\n",
    "# Set path for the director of the data set to be used\n",
    "DATA_DIR = '/Users/aresbandebo/PycharmProjects/Eye_CNN_Testing/Dataset_Eye_Diseases_Classification/'\n",
    "\n",
    "# Set the desired image dimensions for resizing.\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "\n",
    "def create_classes_from_dirs(target_path):\n",
    "    \"\"\"\n",
    "    Scans a target path and dynamically creates Python classes based on the names\n",
    "    of the directories found within it.\n",
    "\n",
    "    Args:\n",
    "        target_path (str): The absolute or relative path to the directory to scan.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the names and class objects that were created.\n",
    "              Returns an empty dictionary if the path does not exist or contains no directories.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to scan for directories in: '{target_path}'\")\n",
    "\n",
    "    # --- 1. Validate the path ---\n",
    "    if not os.path.isdir(target_path):\n",
    "        print(f\"\\nError: The specified path does not exist or is not a directory.\")\n",
    "        print(\"Please make sure the path is correct and accessible.\")\n",
    "        return {}\n",
    "\n",
    "    created_classes = {}\n",
    "\n",
    "    # --- 2. Find all sub-directories ---\n",
    "    try:\n",
    "        # Get a list of all entries in the target_path and filter for directories\n",
    "        dir_names = [name for name in os.listdir(target_path)\n",
    "                     if os.path.isdir(os.path.join(target_path, name))]\n",
    "    except OSError as e:\n",
    "        print(f\"\\nError: Could not access the path. Reason: {e}\")\n",
    "        return {}\n",
    "\n",
    "    if not dir_names:\n",
    "        print(\"\\nNo sub-directories were found in the specified path.\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"\\nFound {len(dir_names)} directories. Creating corresponding classes...\")\n",
    "\n",
    "    # --- 3. Create a class for each directory ---\n",
    "    for dir_name in dir_names:\n",
    "        # Sanitize the directory name to make it a valid Python class name\n",
    "        # (e.g., \"diabetic retinopathy\" -> \"Diabetic_retinopathy\")\n",
    "        class_name = re.sub(r'[^0-9a-zA-Z_]', '_', dir_name).capitalize()\n",
    "\n",
    "        # If the first character is not a letter, prepend 'C' for 'Class'\n",
    "        if not class_name[0].isalpha():\n",
    "            class_name = 'C' + class_name\n",
    "\n",
    "        # Dynamically create the class using the type() function\n",
    "        # Format: type(ClassName, (BaseClasses,), {attributes_and_methods})\n",
    "        new_class = type(class_name, (object,), {\n",
    "            '__doc__': f'Dynamically generated class from the \"{dir_name}\" directory.',\n",
    "            'source_directory': dir_name,\n",
    "            'file_count': len(os.listdir(os.path.join(target_path, dir_name)))\n",
    "        })\n",
    "\n",
    "        # Add the new class to the global scope of this script, making it accessible\n",
    "        globals()[class_name] = new_class\n",
    "        created_classes[class_name] = new_class\n",
    "        print(f\"- Created class: {class_name}\")\n",
    "\n",
    "    return created_classes\n",
    "\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Generate the classes\n",
    "    dynamic_classes = create_classes_from_dirs(DATA_DIR)\n",
    "\n",
    "    # --- Verification Step ---\n",
    "    # This section demonstrates how to use the dynamically created classes.\n",
    "    if dynamic_classes:\n",
    "        print(\"\\n--- Verification ---\")\n",
    "        print(\"The classes have been created and are now available for use.\")\n",
    "        print(\"Let's inspect the first class that was created:\\n\")\n",
    "\n",
    "        # Get the name of the first class from the dictionary\n",
    "        first_class_name = list(dynamic_classes.keys())[0]\n",
    "\n",
    "        # Access the class from the global scope using its name\n",
    "        FirstClass = globals()[first_class_name]\n",
    "\n",
    "        # Create an instance of the class\n",
    "        instance = FirstClass()\n",
    "\n",
    "        # Print some information about the instance and its class\n",
    "        print(f\"Class Name: {first_class_name}\")\n",
    "        print(f\"Instance created: {instance}\")\n",
    "        print(f\"Instance type: {type(instance)}\")\n",
    "        print(f\"Class docstring: {instance.__doc__}\")\n",
    "        print(f\"Original directory name: '{instance.source_directory}'\")\n",
    "        print(f\"Number of files in directory: {instance.file_count}\")\n",
    "\n",
    "    if dynamic_classes:\n",
    "        class_names = list(dynamic_classes.keys())\n",
    "        class_names.sort()\n",
    "        print(\"\\n--- List of All Class Names ---\")\n",
    "        print(\"A list called 'class_names' has been created with the names of all generated classes.\")\n",
    "        print(class_names)"
   ],
   "id": "af3c26f1877d1c7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to scan for directories in: '/Users/aresbandebo/PycharmProjects/Eye_CNN_Testing/Dataset_Eye_Diseases_Classification/'\n",
      "\n",
      "Found 5 directories. Creating corresponding classes...\n",
      "- Created class: Cataract\n",
      "- Created class: Uveitis\n",
      "- Created class: Conjunctivitis\n",
      "- Created class: Eyelid\n",
      "- Created class: Normal\n",
      "\n",
      "--- Verification ---\n",
      "The classes have been created and are now available for use.\n",
      "Let's inspect the first class that was created:\n",
      "\n",
      "Class Name: Cataract\n",
      "Instance created: <__main__.Cataract object at 0x3161e5130>\n",
      "Instance type: <class '__main__.Cataract'>\n",
      "Class docstring: Dynamically generated class from the \"Cataract\" directory.\n",
      "Original directory name: 'Cataract'\n",
      "Number of files in directory: 352\n",
      "\n",
      "--- List of All Class Names ---\n",
      "A list called 'class_names' has been created with the names of all generated classes.\n",
      "['Cataract', 'Conjunctivitis', 'Eyelid', 'Normal', 'Uveitis']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T12:24:17.592370Z",
     "start_time": "2025-11-03T12:24:16.342321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Load and Preprocess the Data\n",
    "def load_data(data_dir, class_names, img_width, img_height):\n",
    "    images = []\n",
    "    labels = []\n",
    "    print(\"Loading image data...\")\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "\n",
    "                # --- THIS IS THE FIX ---\n",
    "                # Convert from BGR (OpenCV) to RGB (Matplotlib)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                # --- END OF FIX ---\n",
    "\n",
    "                img = cv2.resize(img, (img_width, img_height))\n",
    "                images.append(img)\n",
    "                labels.append(class_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "    print(\"Image data loaded successfully.\")\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load the initial data\n",
    "images, labels = load_data(DATA_DIR, class_names, IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "# Normalize pixel values\n",
    "# images = images.astype('float32') / 255.0\n",
    "\n",
    "# --- Model Definition ---\n",
    "def create_ovr_model(input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)):\n",
    "    \"\"\"Creates a binary classifier based on EfficientNetB0 for OvR.\"\"\"\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(name=\"avg_pool\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3, name=\"top_dropout\"),\n",
    "        # Final layer for binary classification (One-vs-Rest)\n",
    "        layers.Dense(1, activation='sigmoid', name=\"pred\")\n",
    "    ])\n",
    "    return model"
   ],
   "id": "ccebb0430f8b8d32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image data...\n",
      "Error loading image /Users/aresbandebo/PycharmProjects/Eye_CNN_Testing/Dataset_Eye_Diseases_Classification/Cataract/.DS_Store: OpenCV(4.12.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "Error loading image /Users/aresbandebo/PycharmProjects/Eye_CNN_Testing/Dataset_Eye_Diseases_Classification/Conjunctivitis/.DS_Store: OpenCV(4.12.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "Error loading image /Users/aresbandebo/PycharmProjects/Eye_CNN_Testing/Dataset_Eye_Diseases_Classification/Eyelid/.DS_Store: OpenCV(4.12.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "Error loading image /Users/aresbandebo/PycharmProjects/Eye_CNN_Testing/Dataset_Eye_Diseases_Classification/Uveitis/.DS_Store: OpenCV(4.12.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "Image data loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T12:24:17.602454Z",
     "start_time": "2025-11-03T12:24:17.600305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Display sample images\n",
    "def show_sample_images(sample_images, sample_labels, class_names_map):\n",
    "    plt.figure(figsize=(12, 120))\n",
    "    for i in range(min(30, len(sample_images))): # Show up to 240 images\n",
    "        plt.subplot(60, 4, i + 1)\n",
    "        plt.imshow(sample_images[i])\n",
    "\n",
    "        # --- START OF FIX ---\n",
    "        label_value = sample_labels[i]\n",
    "\n",
    "        # Check if the label is an integer (like 0, 1) or a string (like 'cataract')\n",
    "        if isinstance(label_value, (int, np.integer)):\n",
    "            # If it's an integer, use it as an index to look up the name\n",
    "            plt.title(class_names_map[label_value])\n",
    "        else:\n",
    "            # If it's already a string, just use it directly\n",
    "            plt.title(label_value)\n",
    "        # --- END OF FIX ---\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(\"Sample Images from Training Set\", fontsize=16, y=0.9)\n",
    "    plt.show()"
   ],
   "id": "d34eeab87dc33d82",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:08:27.957350Z",
     "start_time": "2025-11-03T12:24:17.611358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NEW Step 5 --- One-vs-Rest (OvR) Training Loop ---\n",
    "# This version includes class weights to handle data imbalance and a two-stage fine-tuning process.\n",
    "\n",
    "INITIAL_EPOCHS = 10\n",
    "FINE_TUNE_EPOCHS = 10\n",
    "TOTAL_EPOCHS = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n",
    "\n",
    "for class_name in class_names:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"    Training Model for: {class_name} vs. Rest\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # 1. Create Binary Labels\n",
    "    binary_labels = np.array([1 if label == class_name else 0 for label in labels])\n",
    "\n",
    "    # 2. Split the Data\n",
    "    (X_train_temp, X_test, y_train_temp, y_test) = train_test_split(\n",
    "        images, binary_labels, test_size=0.20, stratify=binary_labels, random_state=42\n",
    "    )\n",
    "    (X_train, X_val, y_train, y_val) = train_test_split(\n",
    "        X_train_temp, y_train_temp, test_size=0.25, stratify=y_train_temp, random_state=42\n",
    "    )\n",
    "    print(f\"Data split complete for '{class_name}'.\")\n",
    "\n",
    "    # --- THE FIX: CALCULATE CLASS WEIGHTS ---\n",
    "    # This balances the loss function, forcing the model to pay attention to the minority class.\n",
    "    weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight = {0: weights[0], 1: weights[1]}\n",
    "    print(f\"Class weights for '{class_name}': {class_weight}\")\n",
    "\n",
    "    # 3. Create and Compile the Model for Stage 1\n",
    "    ovr_model = create_ovr_model()\n",
    "    ovr_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # --- Stage 1: Initial Training (with Class Weights) ---\n",
    "    print(\"\\n--- Starting Stage 1: Initial Training ---\")\n",
    "    history = ovr_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=INITIAL_EPOCHS,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_weight=class_weight,  # <--- Apply the class weights here\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # --- Stage 2: Fine-Tuning ---\n",
    "    print(\"\\n--- Starting Stage 2: Fine-Tuning ---\")\n",
    "    ovr_model.layers[0].trainable = True # Unfreeze the base model\n",
    "\n",
    "    # Unfreeze the top 30 layers for fine-tuning\n",
    "    for layer in ovr_model.layers[0].layers[:-30]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Re-compile with a very low learning rate for fine-tuning\n",
    "    ovr_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Continue training (fine-tuning)\n",
    "    history_fine = ovr_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=TOTAL_EPOCHS,\n",
    "        initial_epoch=INITIAL_EPOCHS, #initial_epoch=history.epoch[-1],\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_weight=class_weight, # <--- Also apply weights during fine-tuning\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # 4. Save and Evaluate the Final Model\n",
    "    model_filename = f\"efficientnet_ovr_{class_name}_finetuned.keras\" # Changed from h5 to keras\n",
    "    ovr_model.save(model_filename)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "    test_loss, test_acc = ovr_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nFinal Test Accuracy for {class_name} vs. Rest: {test_acc:.2%}\")\n",
    "    print(f\"\\nFinal Test Loss for {class_name} vs. Rest: {test_loss:.2}\")\n",
    "\n",
    "print(\"\\nAll models have been trained and fine-tuned successfully!\")"
   ],
   "id": "f4b33d99aefeb48f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "    Training Model for: Cataract vs. Rest\n",
      "==================================================\n",
      "\n",
      "Data split complete for 'Cataract'.\n",
      "Class weights for 'Cataract': {0: 0.6023278370514064, 1: 2.943127962085308}\n",
      "\n",
      "--- Starting Stage 1: Initial Training ---\n",
      "Epoch 1/10\n",
      "39/39 - 16s - 407ms/step - accuracy: 0.7029 - loss: 0.5391 - val_accuracy: 0.9010 - val_loss: 0.3236\n",
      "Epoch 2/10\n",
      "39/39 - 12s - 299ms/step - accuracy: 0.8945 - loss: 0.2527 - val_accuracy: 0.9058 - val_loss: 0.2479\n",
      "Epoch 3/10\n",
      "39/39 - 12s - 297ms/step - accuracy: 0.9082 - loss: 0.2249 - val_accuracy: 0.9420 - val_loss: 0.1925\n",
      "Epoch 4/10\n",
      "39/39 - 12s - 303ms/step - accuracy: 0.9267 - loss: 0.1704 - val_accuracy: 0.9396 - val_loss: 0.1678\n",
      "Epoch 5/10\n",
      "39/39 - 12s - 303ms/step - accuracy: 0.9412 - loss: 0.1583 - val_accuracy: 0.9493 - val_loss: 0.1397\n",
      "Epoch 6/10\n",
      "39/39 - 12s - 299ms/step - accuracy: 0.9549 - loss: 0.1178 - val_accuracy: 0.9614 - val_loss: 0.1225\n",
      "Epoch 7/10\n",
      "39/39 - 12s - 304ms/step - accuracy: 0.9654 - loss: 0.1094 - val_accuracy: 0.9734 - val_loss: 0.1059\n",
      "Epoch 8/10\n",
      "39/39 - 11s - 294ms/step - accuracy: 0.9630 - loss: 0.1038 - val_accuracy: 0.9807 - val_loss: 0.0971\n",
      "Epoch 9/10\n",
      "39/39 - 12s - 299ms/step - accuracy: 0.9638 - loss: 0.0954 - val_accuracy: 0.9807 - val_loss: 0.0960\n",
      "Epoch 10/10\n",
      "39/39 - 12s - 296ms/step - accuracy: 0.9702 - loss: 0.0837 - val_accuracy: 0.9807 - val_loss: 0.0840\n",
      "\n",
      "--- Starting Stage 2: Fine-Tuning ---\n",
      "Epoch 11/20\n",
      "39/39 - 19s - 479ms/step - accuracy: 0.9420 - loss: 0.3229 - val_accuracy: 0.9758 - val_loss: 0.0714\n",
      "Epoch 12/20\n",
      "39/39 - 14s - 352ms/step - accuracy: 0.9364 - loss: 0.2812 - val_accuracy: 0.9807 - val_loss: 0.0721\n",
      "Epoch 13/20\n",
      "39/39 - 14s - 349ms/step - accuracy: 0.9589 - loss: 0.1873 - val_accuracy: 0.9807 - val_loss: 0.0738\n",
      "Epoch 14/20\n",
      "39/39 - 14s - 354ms/step - accuracy: 0.9477 - loss: 0.1931 - val_accuracy: 0.9831 - val_loss: 0.0758\n",
      "Epoch 15/20\n",
      "39/39 - 14s - 357ms/step - accuracy: 0.9557 - loss: 0.1605 - val_accuracy: 0.9855 - val_loss: 0.0768\n",
      "Epoch 16/20\n",
      "39/39 - 14s - 356ms/step - accuracy: 0.9622 - loss: 0.1454 - val_accuracy: 0.9855 - val_loss: 0.0753\n",
      "Epoch 17/20\n",
      "39/39 - 14s - 355ms/step - accuracy: 0.9557 - loss: 0.1457 - val_accuracy: 0.9855 - val_loss: 0.0767\n",
      "Epoch 18/20\n",
      "39/39 - 14s - 361ms/step - accuracy: 0.9581 - loss: 0.1163 - val_accuracy: 0.9855 - val_loss: 0.0779\n",
      "Epoch 19/20\n",
      "39/39 - 14s - 368ms/step - accuracy: 0.9638 - loss: 0.1037 - val_accuracy: 0.9855 - val_loss: 0.0784\n",
      "Epoch 20/20\n",
      "39/39 - 14s - 365ms/step - accuracy: 0.9654 - loss: 0.1056 - val_accuracy: 0.9831 - val_loss: 0.0777\n",
      "Model saved to efficientnet_ovr_Cataract_finetuned.keras\n",
      "\n",
      "Final Test Accuracy for Cataract vs. Rest: 97.59%\n",
      "\n",
      "Final Test Loss for Cataract vs. Rest: 0.072\n",
      "\n",
      "==================================================\n",
      "    Training Model for: Conjunctivitis vs. Rest\n",
      "==================================================\n",
      "\n",
      "Data split complete for 'Conjunctivitis'.\n",
      "Class weights for 'Conjunctivitis': {0: 0.5971153846153846, 1: 3.0742574257425743}\n",
      "\n",
      "--- Starting Stage 1: Initial Training ---\n",
      "Epoch 1/10\n",
      "39/39 - 16s - 401ms/step - accuracy: 0.6634 - loss: 0.6075 - val_accuracy: 0.9034 - val_loss: 0.3506\n",
      "Epoch 2/10\n",
      "39/39 - 12s - 316ms/step - accuracy: 0.8140 - loss: 0.3438 - val_accuracy: 0.9348 - val_loss: 0.2455\n",
      "Epoch 3/10\n",
      "39/39 - 12s - 307ms/step - accuracy: 0.8607 - loss: 0.2814 - val_accuracy: 0.9348 - val_loss: 0.2143\n",
      "Epoch 4/10\n",
      "39/39 - 12s - 309ms/step - accuracy: 0.8929 - loss: 0.2285 - val_accuracy: 0.9444 - val_loss: 0.1909\n",
      "Epoch 5/10\n",
      "39/39 - 12s - 299ms/step - accuracy: 0.9050 - loss: 0.2206 - val_accuracy: 0.9517 - val_loss: 0.1766\n",
      "Epoch 6/10\n",
      "39/39 - 12s - 298ms/step - accuracy: 0.9163 - loss: 0.2124 - val_accuracy: 0.9517 - val_loss: 0.1637\n",
      "Epoch 7/10\n",
      "39/39 - 12s - 301ms/step - accuracy: 0.9082 - loss: 0.1877 - val_accuracy: 0.9589 - val_loss: 0.1547\n",
      "Epoch 8/10\n",
      "39/39 - 12s - 298ms/step - accuracy: 0.9195 - loss: 0.2078 - val_accuracy: 0.9541 - val_loss: 0.1652\n",
      "Epoch 9/10\n",
      "39/39 - 12s - 300ms/step - accuracy: 0.9203 - loss: 0.1720 - val_accuracy: 0.9541 - val_loss: 0.1561\n",
      "Epoch 10/10\n",
      "39/39 - 12s - 302ms/step - accuracy: 0.9380 - loss: 0.1526 - val_accuracy: 0.9541 - val_loss: 0.1548\n",
      "\n",
      "--- Starting Stage 2: Fine-Tuning ---\n",
      "Epoch 11/20\n",
      "39/39 - 19s - 481ms/step - accuracy: 0.8994 - loss: 0.4166 - val_accuracy: 0.8865 - val_loss: 0.2450\n",
      "Epoch 12/20\n",
      "39/39 - 14s - 353ms/step - accuracy: 0.8994 - loss: 0.3606 - val_accuracy: 0.8792 - val_loss: 0.2662\n",
      "Epoch 13/20\n",
      "39/39 - 14s - 350ms/step - accuracy: 0.9211 - loss: 0.2582 - val_accuracy: 0.8937 - val_loss: 0.2458\n",
      "Epoch 14/20\n",
      "39/39 - 14s - 355ms/step - accuracy: 0.9195 - loss: 0.2390 - val_accuracy: 0.9058 - val_loss: 0.2276\n",
      "Epoch 15/20\n",
      "39/39 - 14s - 355ms/step - accuracy: 0.9219 - loss: 0.2397 - val_accuracy: 0.9179 - val_loss: 0.2156\n",
      "Epoch 16/20\n",
      "39/39 - 14s - 357ms/step - accuracy: 0.9171 - loss: 0.2365 - val_accuracy: 0.9227 - val_loss: 0.2138\n",
      "Epoch 17/20\n",
      "39/39 - 14s - 355ms/step - accuracy: 0.9283 - loss: 0.1959 - val_accuracy: 0.9275 - val_loss: 0.2054\n",
      "Epoch 18/20\n",
      "39/39 - 14s - 354ms/step - accuracy: 0.9163 - loss: 0.2128 - val_accuracy: 0.9300 - val_loss: 0.2011\n",
      "Epoch 19/20\n",
      "39/39 - 14s - 351ms/step - accuracy: 0.9163 - loss: 0.2186 - val_accuracy: 0.9324 - val_loss: 0.1980\n",
      "Epoch 20/20\n",
      "39/39 - 14s - 349ms/step - accuracy: 0.9324 - loss: 0.1739 - val_accuracy: 0.9324 - val_loss: 0.1975\n",
      "Model saved to efficientnet_ovr_Conjunctivitis_finetuned.keras\n",
      "\n",
      "Final Test Accuracy for Conjunctivitis vs. Rest: 92.53%\n",
      "\n",
      "Final Test Loss for Conjunctivitis vs. Rest: 0.19\n",
      "\n",
      "==================================================\n",
      "    Training Model for: Eyelid vs. Rest\n",
      "==================================================\n",
      "\n",
      "Data split complete for 'Eyelid'.\n",
      "Class weights for 'Eyelid': {0: 0.6663090128755365, 1: 2.003225806451613}\n",
      "\n",
      "--- Starting Stage 1: Initial Training ---\n",
      "Epoch 1/10\n",
      "39/39 - 16s - 402ms/step - accuracy: 0.6820 - loss: 0.6176 - val_accuracy: 0.8551 - val_loss: 0.4937\n",
      "Epoch 2/10\n",
      "39/39 - 12s - 296ms/step - accuracy: 0.7931 - loss: 0.4285 - val_accuracy: 0.8816 - val_loss: 0.3947\n",
      "Epoch 3/10\n",
      "39/39 - 12s - 295ms/step - accuracy: 0.8414 - loss: 0.3201 - val_accuracy: 0.8986 - val_loss: 0.3354\n",
      "Epoch 4/10\n",
      "39/39 - 11s - 294ms/step - accuracy: 0.8760 - loss: 0.2988 - val_accuracy: 0.9300 - val_loss: 0.2962\n",
      "Epoch 5/10\n",
      "39/39 - 11s - 294ms/step - accuracy: 0.8873 - loss: 0.2592 - val_accuracy: 0.9203 - val_loss: 0.2737\n",
      "Epoch 6/10\n",
      "39/39 - 12s - 295ms/step - accuracy: 0.8921 - loss: 0.2499 - val_accuracy: 0.9275 - val_loss: 0.2474\n",
      "Epoch 7/10\n",
      "39/39 - 11s - 295ms/step - accuracy: 0.9074 - loss: 0.2129 - val_accuracy: 0.9179 - val_loss: 0.2357\n",
      "Epoch 8/10\n",
      "39/39 - 12s - 295ms/step - accuracy: 0.9106 - loss: 0.1997 - val_accuracy: 0.9203 - val_loss: 0.2211\n",
      "Epoch 9/10\n",
      "39/39 - 11s - 293ms/step - accuracy: 0.9171 - loss: 0.1973 - val_accuracy: 0.9396 - val_loss: 0.1923\n",
      "Epoch 10/10\n",
      "39/39 - 12s - 296ms/step - accuracy: 0.9203 - loss: 0.1920 - val_accuracy: 0.9251 - val_loss: 0.2000\n",
      "\n",
      "--- Starting Stage 2: Fine-Tuning ---\n",
      "Epoch 11/20\n",
      "39/39 - 18s - 467ms/step - accuracy: 0.8510 - loss: 0.4684 - val_accuracy: 0.9155 - val_loss: 0.2232\n",
      "Epoch 12/20\n",
      "39/39 - 14s - 351ms/step - accuracy: 0.8720 - loss: 0.3976 - val_accuracy: 0.9155 - val_loss: 0.2206\n",
      "Epoch 13/20\n",
      "39/39 - 13s - 345ms/step - accuracy: 0.8575 - loss: 0.4051 - val_accuracy: 0.9155 - val_loss: 0.2101\n",
      "Epoch 14/20\n",
      "39/39 - 14s - 354ms/step - accuracy: 0.8994 - loss: 0.2987 - val_accuracy: 0.9179 - val_loss: 0.2033\n",
      "Epoch 15/20\n",
      "39/39 - 14s - 358ms/step - accuracy: 0.8881 - loss: 0.3112 - val_accuracy: 0.9082 - val_loss: 0.1979\n",
      "Epoch 16/20\n",
      "39/39 - 14s - 355ms/step - accuracy: 0.8945 - loss: 0.2733 - val_accuracy: 0.9106 - val_loss: 0.1949\n",
      "Epoch 17/20\n",
      "39/39 - 14s - 354ms/step - accuracy: 0.9251 - loss: 0.2113 - val_accuracy: 0.9106 - val_loss: nan\n",
      "Epoch 18/20\n",
      "39/39 - 14s - 355ms/step - accuracy: 0.9050 - loss: 0.2322 - val_accuracy: 0.9082 - val_loss: 0.1879\n",
      "Epoch 19/20\n",
      "39/39 - 14s - 351ms/step - accuracy: 0.8897 - loss: 0.2596 - val_accuracy: 0.9082 - val_loss: 0.1878\n",
      "Epoch 20/20\n",
      "39/39 - 14s - 351ms/step - accuracy: 0.9171 - loss: 0.2171 - val_accuracy: 0.9155 - val_loss: 0.1852\n",
      "Model saved to efficientnet_ovr_Eyelid_finetuned.keras\n",
      "\n",
      "Final Test Accuracy for Eyelid vs. Rest: 93.01%\n",
      "\n",
      "Final Test Loss for Eyelid vs. Rest: 0.17\n",
      "\n",
      "==================================================\n",
      "    Training Model for: Normal vs. Rest\n",
      "==================================================\n",
      "\n",
      "Data split complete for 'Normal'.\n",
      "Class weights for 'Normal': {0: 0.7280187573270809, 1: 1.5964010282776349}\n",
      "\n",
      "--- Starting Stage 1: Initial Training ---\n",
      "Epoch 1/10\n",
      "39/39 - 16s - 401ms/step - accuracy: 0.8124 - loss: 0.3536 - val_accuracy: 0.9493 - val_loss: 0.3215\n",
      "Epoch 2/10\n",
      "39/39 - 12s - 297ms/step - accuracy: 0.9428 - loss: 0.1457 - val_accuracy: 0.9710 - val_loss: 0.1979\n",
      "Epoch 3/10\n",
      "39/39 - 11s - 292ms/step - accuracy: 0.9614 - loss: 0.0992 - val_accuracy: 0.9734 - val_loss: 0.1509\n",
      "Epoch 4/10\n",
      "39/39 - 11s - 294ms/step - accuracy: 0.9750 - loss: 0.0642 - val_accuracy: 0.9807 - val_loss: 0.1141\n",
      "Epoch 5/10\n",
      "39/39 - 11s - 292ms/step - accuracy: 0.9775 - loss: 0.0681 - val_accuracy: 0.9831 - val_loss: 0.0928\n",
      "Epoch 6/10\n",
      "39/39 - 11s - 292ms/step - accuracy: 0.9783 - loss: 0.0642 - val_accuracy: 0.9831 - val_loss: 0.0727\n",
      "Epoch 7/10\n",
      "39/39 - 11s - 294ms/step - accuracy: 0.9783 - loss: 0.0591 - val_accuracy: 0.9879 - val_loss: 0.0603\n",
      "Epoch 8/10\n",
      "39/39 - 11s - 293ms/step - accuracy: 0.9936 - loss: 0.0316 - val_accuracy: 0.9807 - val_loss: 0.0523\n",
      "Epoch 9/10\n",
      "39/39 - 11s - 291ms/step - accuracy: 0.9879 - loss: 0.0387 - val_accuracy: 0.9855 - val_loss: 0.0485\n",
      "Epoch 10/10\n",
      "39/39 - 11s - 293ms/step - accuracy: 0.9855 - loss: 0.0399 - val_accuracy: 0.9879 - val_loss: 0.0440\n",
      "\n",
      "--- Starting Stage 2: Fine-Tuning ---\n",
      "Epoch 11/20\n",
      "39/39 - 18s - 473ms/step - accuracy: 0.9324 - loss: 0.2348 - val_accuracy: 0.9855 - val_loss: 0.0393\n",
      "Epoch 12/20\n",
      "39/39 - 14s - 349ms/step - accuracy: 0.9589 - loss: 0.1525 - val_accuracy: 0.9855 - val_loss: 0.0446\n",
      "Epoch 13/20\n",
      "39/39 - 13s - 345ms/step - accuracy: 0.9718 - loss: 0.1065 - val_accuracy: 0.9855 - val_loss: 0.0486\n",
      "Epoch 14/20\n",
      "39/39 - 14s - 348ms/step - accuracy: 0.9775 - loss: 0.0830 - val_accuracy: 0.9831 - val_loss: 0.0522\n",
      "Epoch 15/20\n",
      "39/39 - 14s - 350ms/step - accuracy: 0.9791 - loss: 0.0919 - val_accuracy: 0.9831 - val_loss: 0.0541\n",
      "Epoch 16/20\n",
      "39/39 - 13s - 346ms/step - accuracy: 0.9815 - loss: 0.0687 - val_accuracy: 0.9807 - val_loss: 0.0553\n",
      "Epoch 17/20\n",
      "39/39 - 13s - 346ms/step - accuracy: 0.9726 - loss: 0.0718 - val_accuracy: 0.9783 - val_loss: 0.0551\n",
      "Epoch 18/20\n",
      "39/39 - 14s - 348ms/step - accuracy: 0.9783 - loss: 0.0622 - val_accuracy: 0.9831 - val_loss: 0.0562\n",
      "Epoch 19/20\n",
      "39/39 - 14s - 354ms/step - accuracy: 0.9807 - loss: 0.0467 - val_accuracy: 0.9831 - val_loss: 0.0563\n",
      "Epoch 20/20\n",
      "39/39 - 14s - 350ms/step - accuracy: 0.9855 - loss: 0.0511 - val_accuracy: 0.9855 - val_loss: 0.0569\n",
      "Model saved to efficientnet_ovr_Normal_finetuned.keras\n",
      "\n",
      "Final Test Accuracy for Normal vs. Rest: 99.76%\n",
      "\n",
      "Final Test Loss for Normal vs. Rest: 0.021\n",
      "\n",
      "==================================================\n",
      "    Training Model for: Uveitis vs. Rest\n",
      "==================================================\n",
      "\n",
      "Data split complete for 'Uveitis'.\n",
      "Class weights for 'Uveitis': {0: 0.5589558955895589, 1: 4.740458015267175}\n",
      "\n",
      "--- Starting Stage 1: Initial Training ---\n",
      "Epoch 1/10\n",
      "39/39 - 372s - 10s/step - accuracy: 0.5443 - loss: 0.7895 - val_accuracy: 0.6135 - val_loss: 0.6397\n",
      "Epoch 2/10\n",
      "39/39 - 919s - 24s/step - accuracy: 0.7110 - loss: 0.4919 - val_accuracy: 0.9227 - val_loss: 0.3476\n",
      "Epoch 3/10\n",
      "39/39 - 63s - 2s/step - accuracy: 0.8027 - loss: 0.3999 - val_accuracy: 0.9275 - val_loss: 0.2831\n",
      "Epoch 4/10\n",
      "39/39 - 12s - 304ms/step - accuracy: 0.8349 - loss: 0.3571 - val_accuracy: 0.9300 - val_loss: 0.2372\n",
      "Epoch 5/10\n",
      "39/39 - 12s - 318ms/step - accuracy: 0.8510 - loss: 0.2998 - val_accuracy: 0.9324 - val_loss: 0.2166\n",
      "Epoch 6/10\n",
      "39/39 - 13s - 332ms/step - accuracy: 0.8712 - loss: 0.2455 - val_accuracy: 0.9348 - val_loss: 0.2033\n",
      "Epoch 7/10\n",
      "39/39 - 12s - 305ms/step - accuracy: 0.8945 - loss: 0.2576 - val_accuracy: 0.9300 - val_loss: 0.1828\n",
      "Epoch 8/10\n",
      "39/39 - 12s - 300ms/step - accuracy: 0.8913 - loss: 0.2179 - val_accuracy: 0.9372 - val_loss: 0.1831\n",
      "Epoch 9/10\n",
      "39/39 - 12s - 302ms/step - accuracy: 0.9082 - loss: 0.2116 - val_accuracy: 0.9324 - val_loss: 0.1810\n",
      "Epoch 10/10\n",
      "39/39 - 12s - 303ms/step - accuracy: 0.9034 - loss: 0.2096 - val_accuracy: 0.9179 - val_loss: 0.2092\n",
      "\n",
      "--- Starting Stage 2: Fine-Tuning ---\n",
      "Epoch 11/20\n",
      "39/39 - 18s - 473ms/step - accuracy: 0.8712 - loss: 0.6272 - val_accuracy: 0.9106 - val_loss: 0.2282\n",
      "Epoch 12/20\n",
      "39/39 - 14s - 349ms/step - accuracy: 0.8824 - loss: 0.5073 - val_accuracy: 0.9034 - val_loss: 0.2277\n",
      "Epoch 13/20\n",
      "39/39 - 14s - 355ms/step - accuracy: 0.8881 - loss: 0.4137 - val_accuracy: 0.9010 - val_loss: 0.2314\n",
      "Epoch 14/20\n",
      "39/39 - 14s - 349ms/step - accuracy: 0.8913 - loss: 0.3638 - val_accuracy: 0.9010 - val_loss: 0.2294\n",
      "Epoch 15/20\n",
      "39/39 - 14s - 347ms/step - accuracy: 0.8994 - loss: 0.3086 - val_accuracy: 0.9058 - val_loss: 0.2316\n",
      "Epoch 16/20\n",
      "39/39 - 14s - 356ms/step - accuracy: 0.8937 - loss: 0.2963 - val_accuracy: 0.9058 - val_loss: 0.2287\n",
      "Epoch 17/20\n",
      "39/39 - 14s - 354ms/step - accuracy: 0.8994 - loss: 0.2481 - val_accuracy: 0.8961 - val_loss: 0.2366\n",
      "Epoch 18/20\n",
      "39/39 - 14s - 352ms/step - accuracy: 0.8953 - loss: 0.2556 - val_accuracy: 0.8937 - val_loss: 0.2383\n",
      "Epoch 19/20\n",
      "39/39 - 14s - 351ms/step - accuracy: 0.8857 - loss: 0.2810 - val_accuracy: 0.8889 - val_loss: 0.2414\n",
      "Epoch 20/20\n",
      "39/39 - 14s - 358ms/step - accuracy: 0.9050 - loss: 0.2389 - val_accuracy: 0.8889 - val_loss: 0.2433\n",
      "Model saved to efficientnet_ovr_Uveitis_finetuned.keras\n",
      "\n",
      "Final Test Accuracy for Uveitis vs. Rest: 91.08%\n",
      "\n",
      "Final Test Loss for Uveitis vs. Rest: 0.23\n",
      "\n",
      "All models have been trained and fine-tuned successfully!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:08:38.797666Z",
     "start_time": "2025-11-03T13:08:38.793817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 6 --- Inference Section: Making a Prediction on a New Image ---\n",
    "\n",
    "def predict_image_ovr(image_path, model_dir='.'):\n",
    "    \"\"\"\n",
    "    Loads a new image, preprocesses it, and predicts its class\n",
    "    using the saved One-vs-Rest models.\n",
    "    \"\"\"\n",
    "    # 1. Load the trained OvR models\n",
    "    ovr_models = {}\n",
    "    for class_name in class_names:\n",
    "        #model_path = os.path.join(model_dir, f\"efficientnet_ovr_{class_name}.h5\")\n",
    "        model_path = os.path.join(model_dir, f\"efficientnet_ovr_{class_name}_finetuned.keras\") # Fix to use finely tuned version\n",
    "        if os.path.exists(model_path):\n",
    "            ovr_models[class_name] = tf.keras.models.load_model(model_path)\n",
    "        else:\n",
    "            print(f\"Warning: Model not found for class '{class_name}' at {model_path}\")\n",
    "            return\n",
    "\n",
    "    # 2. Load and preprocess the new image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    img_array = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    img_array = img_array.astype('float32')\n",
    "\n",
    "    # 3. Get a prediction score from each binary classifier\n",
    "    scores = {}\n",
    "    for class_name, model in ovr_models.items():\n",
    "        score = model.predict(img_array)[0][0]\n",
    "        scores[class_name] = score\n",
    "\n",
    "    # 4. Determine the final prediction\n",
    "    # The class with the highest probability score is the winner.\n",
    "    predicted_class = max(scores, key=scores.get)\n",
    "    highest_score = scores[predicted_class]\n",
    "\n",
    "    print(\"\\n--- Prediction Results ---\")\n",
    "    print(f\"Image: {os.path.basename(image_path)}\")\n",
    "    print(f\"Predicted Condition: {predicted_class.capitalize()}\")\n",
    "    print(f\"Confidence Score: {highest_score:.2%}\")\n",
    "    print(\"\\n--- Individual Model Scores ---\")\n",
    "    for class_name, score in sorted(scores.items(), key=lambda item: item[1], reverse=True):\n",
    "        print(f\"- {class_name.capitalize()}: {score:.2%}\")"
   ],
   "id": "96602f97e1035b3f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:08:47.314941Z",
     "start_time": "2025-11-03T13:08:41.597884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example of how to use the prediction function\n",
    "# Make sure you have an image to test with, for example 'test_image.jpg'\n",
    "predict_image_ovr('/Users/aresbandebo/PycharmProjects/Eye_CNN_Testing/Dataset_Eye_Diseases_Classification/Eyelid/109.jpeg')"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x40ec27ba0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 709ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 729ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 700ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 701ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 702ms/step\n",
      "\n",
      "--- Prediction Results ---\n",
      "Image: 109.jpeg\n",
      "Predicted Condition: Eyelid\n",
      "Confidence Score: 96.04%\n",
      "\n",
      "--- Individual Model Scores ---\n",
      "- Eyelid: 96.04%\n",
      "- Cataract: 5.72%\n",
      "- Uveitis: 5.40%\n",
      "- Conjunctivitis: 1.50%\n",
      "- Normal: 0.19%\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
